<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>seq2seq模型的个人理解</title>
    <link href="/2023/11/26/seq2seq/"/>
    <url>/2023/11/26/seq2seq/</url>
    
    <content type="html"><![CDATA[<p><strong>这篇文章仅供个人学习理解，可能有错误</strong></p><h4 id="encoder-decoder架构"><a href="#encoder-decoder架构" class="headerlink" title="encoder-decoder架构"></a>encoder-decoder架构</h4><p>Encoder-Decoder架构是一种常见的深度学习模型架构，特别是在序列到序列的任务中。</p><ul><li><strong>Encoder</strong>：编码器的主要任务是理解和编码输入数据。在处理文本时，它通常会接收一些列单词，并将这些单词转换为一个连续的向量表示（中间形式），这个向量也被称为 <strong>context</strong>。这个过程可以看作是对输入信息进行压缩，并尽可能保留其核心含义。</li><li><strong>Decoder</strong>：解码器则负责从上述编码生成输出。具体来说，它会接收编码器产生的context，并逐步生成输出序列。每一步都会生成一个新元素，并将其作为下一步的输入之一。</li></ul><p>一个Encoder-Decoder结构为：</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""编码器-解码器架构的基类    Defined in :numref:`sec_encoder-decoder`"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_X<span class="token punctuation">,</span> dec_X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>        enc_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span>        dec_state <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_X<span class="token punctuation">,</span> dec_state<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>在机器翻译任务中，enc_X是源语言, dec_X是对应的目标语言.<br>dec_X直接使用目标语言的原因:为了实现Teacher Forcing 策略.即在训练阶段,使用真实的数据+隐藏状态去预测输出而不是使用预测的输出去预测输出.(防止错上加错)</p><p>在seq2seq模型中,我们也是由编码器-解码器架构实现</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Seq2SeqEncoder</span><span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>Encoder<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""用于序列到序列学习的循环神经网络编码器"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 vocab_size<span class="token punctuation">,</span>                 embed_size<span class="token punctuation">,</span>                 num_hiddens<span class="token punctuation">,</span>                 num_layers<span class="token punctuation">,</span>                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                 <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Seq2SeqEncoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        <span class="token comment"># 嵌入层</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span>                         num_layers<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 输出'X'的形状：(batch_size,num_steps,embed_size)</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token comment"># 在循环神经网络模型中，第一个轴对应于时间步</span>        X <span class="token operator">=</span> X<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># 如果未提及状态，则默认为0</span>        output<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token comment"># output的形状:(num_steps,batch_size,num_hiddens)</span>        <span class="token comment"># state的形状:(num_layers,batch_size,num_hiddens)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>编码器中就正常使用RNN将输入的X变成中间形态.不过seq2seq模型中,解码器使用到的并不是output,而是编码器生成的state的最后一个时间步的隐藏状态,这个隐藏状态中浓缩了前面输入和隐藏状态的所有信息</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Seq2SeqDecoder</span><span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>Decoder<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""用于序列到序列学习的循环神经网络解码器"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 vocab_size<span class="token punctuation">,</span>                 embed_size<span class="token punctuation">,</span>                 num_hiddens<span class="token punctuation">,</span>                 num_layers<span class="token punctuation">,</span>                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                 <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Seq2SeqDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embed_size <span class="token operator">+</span> num_hiddens<span class="token punctuation">,</span>                          num_hiddens<span class="token punctuation">,</span>                          num_layers<span class="token punctuation">,</span>                          dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">init_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> enc_outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 输出'X'的形状：(batch_size,num_steps,embed_size)</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># 广播context，使其具有与X相同的num_steps</span>        context <span class="token operator">=</span> state<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        X_and_context <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        output<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X_and_context<span class="token punctuation">,</span> state<span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># output的形状:(batch_size,num_steps,vocab_size)</span>        <span class="token comment"># state的形状:(num_layers,batch_size,num_hiddens)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>在这个过程中损失函数通过继承CrossEntropyLoss重新定义一个损失函数类.因为在一个序列中,它有可能由实际文本+padding组成.因此我们需要去掉这个padding之后再去计算损失函数.</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#@save</span><span class="token keyword">class</span> <span class="token class-name">MaskedSoftmaxCELoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""带遮蔽的softmax交叉熵损失函数"""</span>    <span class="token comment"># pred的形状：(batch_size,num_steps,vocab_size)</span>    <span class="token comment"># label的形状：(batch_size,num_steps)</span>    <span class="token comment"># valid_len的形状：(batch_size,)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pred<span class="token punctuation">,</span> label<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span><span class="token punctuation">:</span>        weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>label<span class="token punctuation">)</span>        weights <span class="token operator">=</span> sequence_mask<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>reduction <span class="token operator">=</span> <span class="token string">'none'</span>        unweighted_loss <span class="token operator">=</span> <span class="token builtin">super</span><span class="token punctuation">(</span>MaskedSoftmaxCELoss<span class="token punctuation">,</span>                                self<span class="token punctuation">)</span><span class="token punctuation">.</span>forward<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token punctuation">)</span>        weighted_loss <span class="token operator">=</span> <span class="token punctuation">(</span>unweighted_loss <span class="token operator">*</span> weights<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> weighted_loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>反向传播的过程中,编码器和解码器就是通过那个context进行传递梯度.</p>]]></content>
    
    
    
    <tags>
      
      <tag>seq2seq</tag>
      
      <tag>rnn</tag>
      
      <tag>encoder-decoder</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2023/08/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF/"/>
    <url>/2023/08/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/mL_learning/2023-08-24-22-29-24.png"></p><p><img src="/img/mL_learning/2023-08-24-22-38-32.png"></p><p><img src="/img/mL_learning/2023-08-24-22-43-15.png"></p><p>残差网络</p><p><img src="/img/mL_learning/2023-08-24-22-58-02.png"><br><img src="/img/mL_learning/2023-08-24-23-15-12.png"></p><p>$\sigma(F(x)+x)$ 在这其中当$F(x) &#x3D; 0$时不会让训练效果变差,但是如果x中隐含信息,则会在残差块中学到一些有用的信息</p><p>目标检测</p><ul><li>确定输出格式</li><li>确定特征点</li><li>用一个小盒子在图像上滑动,每滑一次,将框住的内容拿去做分类,辨别是否有目标(效率很低)</li></ul><p><img src="/img/mL_learning/2023-08-26-20-30-45.png"></p><p><img src="/img/mL_learning/2023-08-26-20-43-49.png"></p><p><img src="/img/mL_learning/2023-08-26-20-53-30.png"></p><p><img src="/img/mL_learning/2023-08-26-20-57-17.png"></p><p><img src="/img/mL_learning/2023-08-26-23-24-45.png"><br><img src="/img/mL_learning/2023-08-26-23-25-07.png"><br><img src="/img/mL_learning/2023-08-26-23-25-43.png"></p><p><img src="/img/mL_learning/2023-08-26-23-33-20.png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>吴恩达机器学习week4--强化学习</title>
    <link href="/2023/07/29/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <url>/2023/07/29/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>定义: 强化学习不是给定指定的y来告诉程序怎么执行,而是设置奖励函数,当程序执行方向正确时给予奖励,错误时给予惩罚来进行修正程序的执行<br>过程</p><ul><li>状态</li><li>动作</li><li>奖励</li><li>下一个状态</li></ul><p>回报:<br><img src="/img/mL_learning/2023-07-29-15-44-31.png"><br>设置不同的gammaγ值<br><img src="/img/mL_learning/2023-07-29-15-53-13.png"></p><p>几个(MDP markov Decision Process 马可夫决策)的例子<br><img src="/img/mL_learning/2023-07-29-16-19-26.png"><br>MDP指的是未来的状态只与当前的状态有关,无关其他时候的任何状态</p><p>Q-function(Q函数)<br><img src="/img/mL_learning/2023-07-29-16-37-58.png"></p><p>贝尔曼方程<br><img src="/img/mL_learning/2023-07-29-17-17-05.png"></p><p>在强化学习过程中,状态的奖励都是随机的,因此找一个具体的最大值是没有意义的,因此改进一下贝尔曼方程<br>改成找后续状态的最大平均值(也就是期望E,在概率中用期望表示平均值)<br>为啥是期望呢,是因为强化学习过程中,智能体有概率走错方向<br><img src="/img/mL_learning/2023-07-29-17-55-09.png"></p><p>深度强化<br>输入状态输出$Q(s,a)$<br><img src="/img/mL_learning/2023-07-29-18-11-27.png"></p><p>DQN(Deep-Q-network)<br><img src="/img/mL_learning/2023-07-29-18-28-28.png"></p><p>优化方法</p><ul><li>修改神经网络的输出层,将一个神经元改成所有action的$Q(s,a)$</li><li>修改计算方法,引入$\epsilon$ (因为神经网络中的参数都是随机的,因此很可能出现某个行为的期望非常低,但是这实际上可能是个好的action,但是因为期望很低,智能体是不会选择它的,所以引入随机选择action的可能)(还可以在开始的时候将$\epsilon$设置比较高的值,随着算法运行,逐步减少$&#x2F;epsilon$)<br><img src="/img/mL_learning/2023-07-29-18-40-45.png"></li><li>Mini-batch</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>理解前向传播与后向传播的关系，串联神经网络流程</title>
    <link href="/2023/07/29/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <url>/2023/07/29/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/</url>
    
    <content type="html"><![CDATA[<p>在神经网络算法中，我们的训练集会有input和output(实际上就是y)<br>因此首先用前向传播获得${\hat y}$ 这个是预估的输出值,此时可以使用损失函数平方损失函数$J &#x3D; \frac{1}{2}{\sum_{i &#x3D; 1}^{m}(y-\hat y)^2}$来计算误差 或者其他损失函数(比如sigmoid函数的对数损失函数)<br>当误差比较大时就说明此时的参数$w$和$b$不合适,需要重新调整.<br>(这个后续让我想到了梯度下降)<br>以下面的神经网络为例</p><p><img src="/img/mL_learning/%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="神经网络图片"></p><p>计算$w_5$对整体结果的影响,使用$L$对$w_5$求偏导(为啥是求导嘞:因为输入是固定的,这里的$h_1$ 和 $h_2$都是上一层的输出层,结果已经由上一层确定了,所以$w, b$是变量)<br>在这里我们先求$L$对$w$和$b$的导数用于梯度下降更新$w和b$<br>令$ a &#x3D; \hat y$<br>$$<br>    z^{[l]} &#x3D; w^{[l]} \cdot a^{[l-1]} + b^{[l]}<br>$$<br>$$<br>    a^{[l]} &#x3D; \hat y &#x3D; \frac{1}{1+e^{-z^{[l]}}}<br>$$<br>$$<br>    L &#x3D; \frac{1}{2}(y_i - \hat {y_i})^2 &#x3D; \frac{1}{2}(y_i - a_i)^2<br>$$<br>$$<br>    \frac{\partial L}{\partial a_i} &#x3D; y_i - a_i<br>$$<br>$$<br>    \frac{\partial a_i}{\partial z_i} &#x3D; a_i \cdot (1-a_i)<br>$$</p><p>$$<br>    \frac{\partial z_i}{\partial w_i} &#x3D; x<br>$$<br>$$<br>    \frac{\partial J}{\partial w} &#x3D; \frac{\partial \frac{1}{2}{\sum_{i &#x3D; 1}^{m}(y-{\frac{1}{1+e^{-(x \cdot w+b)}}})}}{\partial w}&#x3D; \frac{\partial L}{\partial a}\cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w}<br>$$<br>$$<br>    \frac{\partial L}{\partial w} &#x3D; (y-a) \cdot a \cdot (1-a) \cdot x<br>$$<br>在上面的公式中带下标$i$的表示单个变量,不带下标的表示向量</p><p>接着使用梯度下降方程来更新w<br>$$<br>    w &#x3D; w-\alpha \cdot \frac{\partial J}{\partial w}<br>$$</p><p>$b$更新的式子同理,甚至比$w$简单,就不写了</p><p>这是倒数第一个隐藏层中参数$w$的更新公式,在倒数第二个隐藏层中的$w$ 则需要考虑到$L_1 和 L_2$对它都有影响这也只是多了几层链式求导罢了</p><p>在这里我给自己梳理了神经网络中前向传播和后向传播的关系,之前我一直割裂得看二者的关系,导致昨天听网课的时候完全听不懂这些关系式子是如何来的.</p><p>参考:<br>[1] 吴恩达深度学习网课<br>[2] <a href="https://www.cnblogs.com/charlotte77/p/5629865.html">https://www.cnblogs.com/charlotte77/p/5629865.html</a></p><p>[2]参考的是这篇文章:<br><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>前向传播</tag>
      
      <tag>后向传播</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>吴恩达深度学习NLP入门</title>
    <link href="/2023/07/23/NLP%E5%85%A5%E9%97%A8/"/>
    <url>/2023/07/23/NLP%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<p>RNN的出现<br>  传统的神经网络都是一个X对应一个Y,输入和输出一一对应,不同的输入之间没有关系.但是对于序列来说(文字序列,音频序列)一个个输入之间不是孤立的,而是有相对应关系的,因此引入RNN</p><p>RNN的公式统一定义为$h_t &#x3D; f(x_t, h_{t-1}; \theta)$<br>其中$h_t$的每一步计算都是由其当前的输入$x_t$和上一步的输入$h_{t-1}$决定的,$\theta$则是训练时使用的参数.<br>因此$\theta$是参数,因此在传播过程中就需要训练它使其成为合适的参数(前向传播+反向传播)<br>$$<br>\frac{\partial h_t}{\partial \theta} &#x3D; \frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial h_{t-1}}{\partial \theta} + \frac{\partial h_t}{\partial \theta}<br>$$<br>由上式中我们可以看到$\frac{\partial h_t}{\partial \theta}$是前一个状态$\frac{\partial h_{t-1}}{\partial \theta}$对$\theta$和当前状态对$\theta$的函数.在上面的式子中$|\frac{\partial h_t}{\partial h_{t-1}}|&gt;1$ 则表示增强前面的梯度, $\frac{\partial h_t}{\partial h_{t-1}}|&lt;1$则表示减弱前面的梯度.模型很难做到在1附近摆动,从而保持梯度信息,因此梯度消失或者梯度下降在所难免.</p><p><strong>对于梯度消失&#x2F;爆炸的理解</strong><br>在RNN中谈论梯度消失不是指总梯度&#x3D;0,而是对于前面的总项$\frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial h_{t-1}}{\partial \theta} \rarr o$ 这表示前面的梯度信息已经对当前梯度更新不起作用了, 在序列中表示前面的输入对后面输入的依赖关系已经消失了.</p><p>辅助公式<br>$$<br>tanh(x) &#x3D; 2\sigma(2x)-1<br>$$<br>$$<br>\sigma(x) &#x3D; \frac{1}{2}(tanh(\frac{x}{2})+1)<br>$$<br>$$<br>(tanhx)’ &#x3D; 1-{tanh}^2x<br>$$<br>$$<br>\sigma ‘ &#x3D; \sigma(x)(1-\sigma(x))<br>$$</p><p>原始RNN<br>$$<br>  h_t &#x3D; tanh(W_{x_t} + Uh_{t-1}+b)<br>$$<br>下面可以计算一下$ \frac{\partial h_t}{\partial h_{t-1}}$<br>$$<br> \frac{\partial h_t}{\partial h_{t-1}} &#x3D; (tanh(W_{x_t} + Uh_{t-1}+b))’\cdot U &#x3D; (1-{tanh}^2(W_{x_t} + Uh_{t-1}+b))*U &#x3D; (1-{h_t}^2)*U<br>$$<br>U的值是不确定的,因此$ \frac{\partial h_t}{\partial h_{t-1}}$的值可能大于1也可能小于1,因此是存在梯度消失&#x2F;爆炸的风险.因为激活函数使用tanh,因此当|U|非常大时,会将$h_t$限制在[-1,1]之间,使得$(1-{h_t}^2)*U$有界.<br>这也是使用tanh作为激活函数的原因,它能缓解梯度爆炸。如果我们使用relu激活函数时，当|U|非常大时，$h_t$也会非常大,$\frac{\partial h_t}{\partial h_{t-1}}$也很容易非常大,因此梯度爆炸风险非常高.而tanh激活函数可以将$h_t$限制在[-1,1]之间,大大降低了梯度爆炸的风险.</p><p>对于LSTM:<br>$$<br>f_t &#x3D; \sigma(W_{fx_t} + U_{f}h_{t-1}+b_f)<br>$$<br>$$<br>  i_t &#x3D; \sigma(W_{ix_t}+U_ih_{i-1}+b_i)<br>$$<br>$$<br>  o_t &#x3D; \sigma(W_{ox_t}+U_ch_{t-1}+b_c)<br>$$<br>$$<br>  \hat{c}<em>t &#x3D; tanh(W</em>{ox_t} + U_oh_{t-1}+b_c)<br>$$</p><p>$$<br>  c_t &#x3D; f_t \cdot c_{t-1} + i_t \cdot \hat{c}_t<br>$$<br>$$<br>  h_t &#x3D; o_t \cdot tanh(c_t)<br>$$</p><p>GRU:<br>$$<br>  z_t&#x3D;\sigma(W_{zx_t}+U_zh_{t-1}+b_z)<br>$$<br>$$<br>  r_t &#x3D; \sigma(W_{rx_t}+U_rh_{t-1}+b_r)<br>$$<br>$$<br>  \hat{h}<em>t &#x3D; tanh(W</em>{hx_t}+U_h(r_t\cdot h_{t-1})+b_c)<br>$$<br>$$<br>  h_t &#x3D; (1-z_t)\cdot h_{t-1}+z_t \cdot \hat h_t<br>$$</p><p><img src="/img/mL_learning/2023-08-29-21-34-52.png"></p><hr><p><img src="/img/mL_learning/2023-08-29-22-01-17.png"><br>W参数一样，激活函数中的参数不一样<br><img src="/img/mL_learning/2023-08-29-21-57-41.png"></p><p><img src="/img/mL_learning/2023-08-29-22-08-13.png"></p><p><img src="/img/mL_learning/2023-08-29-22-16-35.png"><br>a按时间顺序（倒序）来更新</p><p>梯度爆炸使用梯度修建的方式来缓解</p><p><img src="/img/mL_learning/2023-08-29-22-26-56.png"></p><p><img src="/img/mL_learning/2023-08-30-09-13-13.png"><br>由上图可以看出来, 后面得到的概率全是在前面的影响下得出来的.</p><p><img src="/img/mL_learning/2023-08-30-09-52-31.png"></p><p><img src="/img/mL_learning/2023-08-31-16-22-23.png"></p><p><img src="/img/mL_learning/2023-08-31-16-57-41.png"></p><p><img src="/img/mL_learning/2023-08-31-17-41-18.png"><br>还是没懂为什么使用余弦函数可以很好的预测词汇之间的关系</p><p><a href="https://www.zhihu.com/tardis/zm/art/43396514?source_id=1003">https://www.zhihu.com/tardis/zm/art/43396514?source_id=1003</a></p><p>对于ship-gram算法的实现过程不是很理解<br><a href="https://blog.csdn.net/weixin_41843918/article/details/90312339">https://blog.csdn.net/weixin_41843918/article/details/90312339</a></p><p>glove算法</p><p><img src="/img/mL_learning/2023-09-01-12-28-37.png"></p><p><img src="/img/mL_learning/2023-09-01-15-45-14.png"></p><p><img src="/img/mL_learning/2023-09-01-16-24-09.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>RNN</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/07/19/hello-world/"/>
    <url>/2023/07/19/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
