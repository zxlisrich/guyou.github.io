<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>LLaMA模型详解</title>
    <link href="/2024/03/29/LLaMA/LLaMA/"/>
    <url>/2024/03/29/LLaMA/LLaMA/</url>
    
    <content type="html"><![CDATA[<h3 id="LLaMA模型详解"><a href="#LLaMA模型详解" class="headerlink" title="LLaMA模型详解"></a>LLaMA模型详解</h3><h4 id="Transformer与LLaMA之间的结构差异"><a href="#Transformer与LLaMA之间的结构差异" class="headerlink" title="Transformer与LLaMA之间的结构差异"></a>Transformer与LLaMA之间的结构差异</h4><p>Transformer是Encoder+Decoder结构的,LLaMA是Decoder-only结构,并且LLaMA整体的结构就是Transformer的Decoder的变种.<br><img src="/img/LLaMA/2024-03-23-15-45-02.png"><br>decoder-block的差异:</p><ul><li>二者都会对输入序列进行嵌入化处理,不过在Transformer的Decoder中会在嵌入化处理后加入绝对位置信息,而LLaMa的嵌入层之后是RMS层</li><li>Transformer中都是在每一模块之后添加一个归一化层,而LLaMA中则是将归一化层放在每一个模块输入之前,并用RSM Norm代替Layer Norm </li><li>Transformer中使用的是经典的MHA,LLaMA中使用的是GMA+KV Cache,并且将位置信息加入在Q K中</li><li>在FFN中LLaMa用SwiGLU替换了原来的ReLU</li></ul><h4 id="Models-LLaMA1-LLaMA2"><a href="#Models-LLaMA1-LLaMA2" class="headerlink" title="Models (LLaMA1 LLaMA2)"></a>Models (LLaMA1 LLaMA2)</h4><p><img src="/img/LLaMA/2024-03-23-16-15-40.png"></p><h4 id="RMS"><a href="#RMS" class="headerlink" title="RMS"></a>RMS</h4><h5 id="为什么要归一化"><a href="#为什么要归一化" class="headerlink" title="为什么要归一化"></a>为什么要归一化</h5><p>举一个很明显的例子—-手机制作过程<br>从原材料-&gt;硬件(主板,电池等等)-&gt;软件(qq, vx等等)-&gt;输出,然后与target对比,计算loss反馈到硬件(主板调整一下,电池调整一下各个硬件都调整)然后来到软件它就完全不认识这些硬件了,它得重新认识这些硬件,然后重新生成一批参数,<strong>这会导致整个训练过程非常缓慢,因此归一化的出现可以让参数变化幅度在一个范围内,那下一层就不用花大力气去重新生成参数</strong><br><img src="/img/LLaMA/2024-03-23-16-52-51.png"></p><p><strong>专业化解释</strong></p><ul><li>神经元对于一个数据项的输出依赖于输入数据项的特征（以及神经元的参数）。</li><li>我们可以将输入到一个神经元的数据视为前一个线性层的输出。如果前一层在梯度下降更新权重后，其输出发生了剧烈变化，那么下一层的输入也会发生剧烈变化，因此在梯度下降的下一步中，它将被迫同样剧烈地调整自己的权重。</li><li>神经网络内部节点（神经元）的分布发生变化的现象被称为内部协变量偏移。我们希望避免这种现象，因为它会使得网络训练变慢，由于前一层输出的剧烈变化，神经元被迫在一个或另一个方向上剧烈调整它们的权重。</li></ul><h5 id="Layer-Norm"><a href="#Layer-Norm" class="headerlink" title="Layer Norm"></a>Layer Norm</h5><p><strong>Layer Norm与Batch Norm的区别</strong> </p><ul><li>Layer Norm是针对一个样本进行归一化</li><li>Batch Norm是针对一个特征进行归一化</li></ul><p><strong>在NLP中使用Layer Norm的原因</strong></p><ul><li>Batch Norm很适合使用在图像领域,因为图像的输入是一个矩阵,使用pytorch对矩阵的一行或者一列进行处理是很方便的一件事情</li><li>在NLP领域中,样本就是一段文本,每一个词汇就是一个样本,因此使用Batch Norm没有啥意义,所以使用Layer Norm来对整个样本归一化</li></ul><p><strong>Layer Norm如何进行归一化</strong><br>使用的是如下公示<br><img src="/img/LLaMA/2024-03-24-11-09-02.png"><br>统计学中有个定理是如果$x$的平均数为$\mu$,方差为${\sigma}^2$  也就是 $x\sim U(\mu, {\sigma}^2)$ 可以通过$$ \frac{x-\mu}{\sqrt{\sigma} } &#x3D; z\sim U(0,1) $$将$x$变为0-1正态分布.</p><ul><li>$\gamma$是重新放缩因子,它控制着归一化后的数据的新尺度,如果$\gamma &gt; 1$ 数据的变化范围会增加, $\gamma &lt; 1$ 则变化范围会减少</li><li>$\beta$是重新平移因子,它允许模型在必要时将数据的平均值偏移</li></ul><h5 id="RMS-Root-Mean-Square-Normalization-均方根"><a href="#RMS-Root-Mean-Square-Normalization-均方根" class="headerlink" title="RMS(Root Mean Square Normalization 均方根)"></a>RMS(Root Mean Square Normalization 均方根)</h5><p>上文提到,LayerNorm成功的原因是其re-centering和re-scaling的不变性属性.前者使模型对输入和权重的移位噪声不敏感,后者在输入和权重随机缩放时保持输出表示不变.在RMS中则是认为LayerNorm成功的原因是re-scaling,与re-centering无关<br>因此在RMSNorm中<br><img src="/img/LLaMA/2024-03-24-12-26-16.png"><br>使用RMSNorm的好处</p><ul><li>与LayerNorm相比 RMSNorm需要较少的计算量,因为方差$\sigma &#x3D; \sqrt{\frac{1}{n}\sum_{i&#x3D;1}^{n}(a_i-\mu)^2}$ </li><li>RMSNorm在图像领域也能很好地应用</li></ul><h4 id="RoPE"><a href="#RoPE" class="headerlink" title="RoPE"></a>RoPE</h4><p><strong>positional encoding</strong></p><ul><li>绝对位置编码(absolute positional encoding):绝对位置编码是固定的向量,它们被添加到一个token的embedding中用以表示该token在句子中的位置</li><li>相对位置编码(relative positional encoding):相对位置编码一次处理两个token,我们在计算注意力时使用到相对位置编码:因为注意力机制捕捉两个token之间的强度关系,而相对位置编码可以告诉注意力机制这两token之间的关系.<br>下面这张图表示在使用绝对位置编码或相对位置编码时attention的计算公式.<br>绝对位置编码在生成token的embedding时就已经加入到embedding中了,而相对位置编码则是存在一个相对位置信息的向量,将其与K相加再来求attention<br><img src="/img/LLaMA/2024-03-24-15-43-17.png"></li></ul><p><strong>RoPE(Rotary Position Embedding旋转位置编码)</strong><br>旋转位置编码实际上是一种通过绝对位置编码的方式实现相对位置编码的方法.RoPE的作者实际上是假设$Q$与$K$之间的内积可以被一个函数$g$表示,而函数$g$的输入为词嵌入$x_m$,$x_n$和相对位置信息$m-n$:<br>$$&lt;f_q(x_m,m), f_k(x_n, n)&gt; &#x3D; g(x_m, x_n, m-n)$$<br>作者提出了满足上述关系的$f$和$g$的形式:<br>$$<br>    f_q(x_m,m) &#x3D; (W_qx_m)e^{im\theta} \tag{1}<br>$$</p><p>$$<br>    f_k(x_n, n) &#x3D; (W_kx_n)e^{in\theta} \tag{2}<br>$$</p><p>$$<br>    g(x_m, x_n, m-n) &#x3D; Re[(W_qx_m)(W_kx_n)e^{i(m-n)\theta}] \tag{3}<br>$$<br>由<a href="https://zhuanlan.zhihu.com/p/642884818">一文看懂 LLaMA 中的旋转式位置编码（Rotary Position Embedding）</a>可得<br>$$<br>    f_q(x_m, m) &#x3D; (W_qx_m)e^{im\theta} &#x3D; q_me^{im\theta} &#x3D; \begin{pmatrix}<br>cos(m\theta) &amp; -sin(m\theta)\<br>sin(m\theta) &amp; cos(m\theta)\<br>\end{pmatrix} \begin{pmatrix}<br>    q^{(1)}_m \ q^{(2)}_m<br>\end{pmatrix}<br>$$<br>$f_k(x_n, n)$可得相似等式.</p><p>也就是说在$q$ 和 $k$中都含有相应的位置信息,即$q$ 或者$k$乘以一个旋转矩阵(这也是旋转矩阵名字的由来)<br>现在由旋转矩阵的性质:${R\alpha}^T &#x3D; R(-\alpha)$ $R\alpha \cdot R\beta &#x3D; R(\alpha + \beta)$得<br>$$<br>\begin{aligned}<br>&lt;R\alpha X, R\beta Y&gt; &amp;&#x3D; (R\alpha X)^T \cdot R\beta Y \<br>&amp;&#x3D;X^T \cdot (R\alpha)^T \cdot R\beta Y \<br>&amp;&#x3D;X^T \cdot R(-\alpha)R\beta Y \<br>&amp;&#x3D;X^T \cdot R(\alpha - \beta)Y \<br>&amp;&#x3D;&lt;X,  R(\alpha - \beta)Y&gt;<br>\end{aligned}<br>$$<br>上式中$X&#x3D;q, Y&#x3D;k, \alpha&#x3D;m\theta, \beta&#x3D;n\theta$,完美体现了<strong>通过绝对位置编码的方式实现相对位置编码</strong><br>作者计算了通过改变两个token之间的距离得到的内积的上限，并证明它随着相对距离的增长而衰减。<br>这意味着，当两个token之间的距离增大时，用RoPE的两个token之间的“关系强度”在数值上会变小。这表明旋转位置编码是完全有效的。<br><img src="/img/LLaMA/2024-03-24-18-18-00.png"></p><h4 id="KV-Cache"><a href="#KV-Cache" class="headerlink" title="KV-Cache"></a>KV-Cache</h4><h5 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h5><p>自注意力可以让模型知道每个词之间的相关性<br>$Attention(Q,K,V) &#x3D; softmax(\frac{QK^T}{\sqrt{d_k}})V$<br><img src="/img/LLaMA/2024-03-24-21-33-50.png"><br>上图中得到的$Attention$矩阵表示的是各个词之间的相近程度</p><h5 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h5><p>$MultiHead(Q,K,V) &#x3D; Concat(head1, …head_h)W^o$<br><img src="/img/LLaMA/2024-03-24-21-42-46.png"></p><h5 id="Next-Token-Prediction-Task"><a href="#Next-Token-Prediction-Task" class="headerlink" title="Next Token Prediction Task"></a>Next Token Prediction Task</h5><p>Next Token Prediction Task是很多大语言模型中使用的方法即给定前t个词,让模型预测第t+1处的词,每一次的输出都会与输入拼接起来,作为下一个词的输入.这导致输入序列越来越长.所以我们引入KV Cache来解决这个问题<br><img src="/img/LLaMA/2024-03-24-22-23-40.png"><br>按照上图的流程得到词与词之间相关性的分数$Attention$矩阵,将$Attention$传入Linear+SoftMax层得到预测的token<br>下面是自注意力机制生成token计算$Attention$的过程<br><strong>计算Attention1</strong><br><img src="/img/LLaMA/2024-03-24-22-41-50.png"></p><p><strong>计算Attention2</strong><br><img src="/img/LLaMA/2024-03-24-22-42-42.png"></p><p><strong>计算Attention3</strong><br><img src="/img/LLaMA/2024-03-24-22-43-03.png"></p><p><strong>计算Attention4</strong><br><img src="/img/LLaMA/2024-03-24-22-43-37.png"></p><p>实际上我们每次生成token都是关心最新的$Attention$ 前面的$Attention$都已经在生成相应的token时已经计算过了,这部分在每次生成新的token时都会重复计算,如果能把这部分缓存起来,则可以省下很大一部分计算.</p><p>下面是使用KV Cache技术的Self-Attention<br><strong>计算Attention1</strong><br>此时没有任何输入,所以将token1放进来就好<br><img src="/img/LLaMA/2024-03-24-22-55-55.png"></p><p><strong>计算Attention2</strong><br>在计算$Attention2$时 $Q$与之前相比发生了改变,它只使用最新得到的token,而$K,V$则需要保留前面的token参与运算(这也是叫KV Cache这个名字的原因吧)<br><img src="/img/LLaMA/2024-03-24-23-03-12.png"><br><strong>计算Attention3</strong><br><img src="/img/LLaMA/2024-03-24-23-03-58.png"><br><strong>计算Attention4</strong><br><img src="/img/LLaMA/2024-03-24-23-04-46.png"></p><h4 id="GQA"><a href="#GQA" class="headerlink" title="GQA"></a>GQA</h4><h5 id="MHA-Multi-Head-Attention"><a href="#MHA-Multi-Head-Attention" class="headerlink" title="MHA(Multi-Head Attention)"></a>MHA(Multi-Head Attention)</h5><p>在transformer中的Encoder和Decoder模块都使用了Multi-Head Attention，它本质上还是Self-Attention。只是Transformer的作者考虑到单个Self-Attention可学习的参数较少，泛化性较弱，因此他考虑将$Q, K, V$投影到低维,投影h次(头的个数),然后计算相应的$Attention$,最后将$Attention$concat起来然后再投影回去得到最终的输出.<br><img src="/img/LLaMA/2024-03-25-18-09-17.png"><br>简单来说计算公式如下:<br>$$<br>    MultiHead(Q, K, V) &#x3D; Concat(head_1, …head_h)W^o<br>$$<br>特点:</p><ul><li>MHA 使用多个“头”同时处理输入的不同方面，就像多任务侦探一样。</li><li>可以把它想象成阅读一份有多个标题的报纸——MHA 捕捉到了文本的各种细微差别。</li><li>MHA 提供高质量的结果，但由于并行处理，计算成本可能很高。</li></ul><h5 id="GQA-Grouped-query-Attention"><a href="#GQA-Grouped-query-Attention" class="headerlink" title="GQA(Grouped-query Attention)"></a>GQA(Grouped-query Attention)</h5><p>LLaMA使用GQA来提升模型的计算效率,下面这张图能看出MHA和GQA的区别(就是$K,V$少投影几条)<br>特点:</p><ul><li>GQA 通过对查询进行分组并将注意力集中在这些组上来平衡速度和细节。</li><li>可以将其视为将团队划分为较小的单位来处理特定任务，但同时进行整体协作。  </li><li>GQA 提供比 MHA 更快的处理速度，同时通过群体内的集中注意力保留一些细节。</li></ul><p>(让我想起操作系统的线程)<br><img src="/img/LLaMA/2024-03-24-23-13-48.png"></p><p>[1] <a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo">LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU</a><br>[2] <a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0">Transformer论文逐段精读【论文精读】</a><br>[3] <a href="https://spaces.ac.cn/archives/8265/comment-page-1">Transformer升级之路：2、博采众长的旋转式位置编码</a><br>[4] <a href="https://zhuanlan.zhihu.com/p/642884818">一文看懂 LLaMA 中的旋转式位置编码（Rotary Position Embedding）</a><br>[5] <a href="https://iamshobhitagarwal.medium.com/navigating-the-attention-landscape-mha-mqa-and-gqa-decoded-288217d0a7d1">Navigating the Attention Landscape: MHA, MQA, and GQA Decoded</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLMs</tag>
      
      <tag>LLaMA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>seq2seq模型的个人理解</title>
    <link href="/2023/11/26/seq2seq/"/>
    <url>/2023/11/26/seq2seq/</url>
    
    <content type="html"><![CDATA[<p><strong>这篇文章仅供个人学习理解，可能有错误</strong></p><h4 id="encoder-decoder架构"><a href="#encoder-decoder架构" class="headerlink" title="encoder-decoder架构"></a>encoder-decoder架构</h4><p>Encoder-Decoder架构是一种常见的深度学习模型架构，特别是在序列到序列的任务中。</p><ul><li><strong>Encoder</strong>：编码器的主要任务是理解和编码输入数据。在处理文本时，它通常会接收一些列单词，并将这些单词转换为一个连续的向量表示（中间形式），这个向量也被称为 <strong>context</strong>。这个过程可以看作是对输入信息进行压缩，并尽可能保留其核心含义。</li><li><strong>Decoder</strong>：解码器则负责从上述编码生成输出。具体来说，它会接收编码器产生的context，并逐步生成输出序列。每一步都会生成一个新元素，并将其作为下一步的输入之一。</li></ul><p>一个Encoder-Decoder结构为：</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""编码器-解码器架构的基类    Defined in :numref:`sec_encoder-decoder`"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_X<span class="token punctuation">,</span> dec_X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>        enc_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span>        dec_state <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_X<span class="token punctuation">,</span> dec_state<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>在机器翻译任务中，enc_X是源语言, dec_X是对应的目标语言.<br>dec_X直接使用目标语言的原因:为了实现Teacher Forcing 策略.即在训练阶段,使用真实的数据+隐藏状态去预测输出而不是使用预测的输出去预测输出.(防止错上加错)</p><p>在seq2seq模型中,我们也是由编码器-解码器架构实现</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Seq2SeqEncoder</span><span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>Encoder<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""用于序列到序列学习的循环神经网络编码器"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 vocab_size<span class="token punctuation">,</span>                 embed_size<span class="token punctuation">,</span>                 num_hiddens<span class="token punctuation">,</span>                 num_layers<span class="token punctuation">,</span>                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                 <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Seq2SeqEncoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        <span class="token comment"># 嵌入层</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span>                         num_layers<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 输出'X'的形状：(batch_size,num_steps,embed_size)</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token comment"># 在循环神经网络模型中，第一个轴对应于时间步</span>        X <span class="token operator">=</span> X<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># 如果未提及状态，则默认为0</span>        output<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token comment"># output的形状:(num_steps,batch_size,num_hiddens)</span>        <span class="token comment"># state的形状:(num_layers,batch_size,num_hiddens)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>编码器中就正常使用RNN将输入的X变成中间形态.不过seq2seq模型中,解码器使用到的并不是output,而是编码器生成的state的最后一个时间步的隐藏状态,这个隐藏状态中浓缩了前面输入和隐藏状态的所有信息</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Seq2SeqDecoder</span><span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>Decoder<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""用于序列到序列学习的循环神经网络解码器"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 vocab_size<span class="token punctuation">,</span>                 embed_size<span class="token punctuation">,</span>                 num_hiddens<span class="token punctuation">,</span>                 num_layers<span class="token punctuation">,</span>                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                 <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Seq2SeqDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embed_size <span class="token operator">+</span> num_hiddens<span class="token punctuation">,</span>                          num_hiddens<span class="token punctuation">,</span>                          num_layers<span class="token punctuation">,</span>                          dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">init_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> enc_outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 输出'X'的形状：(batch_size,num_steps,embed_size)</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># 广播context，使其具有与X相同的num_steps</span>        context <span class="token operator">=</span> state<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        X_and_context <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        output<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X_and_context<span class="token punctuation">,</span> state<span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># output的形状:(batch_size,num_steps,vocab_size)</span>        <span class="token comment"># state的形状:(num_layers,batch_size,num_hiddens)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>在这个过程中损失函数通过继承CrossEntropyLoss重新定义一个损失函数类.因为在一个序列中,它有可能由实际文本+padding组成.因此我们需要去掉这个padding之后再去计算损失函数.</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#@save</span><span class="token keyword">class</span> <span class="token class-name">MaskedSoftmaxCELoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""带遮蔽的softmax交叉熵损失函数"""</span>    <span class="token comment"># pred的形状：(batch_size,num_steps,vocab_size)</span>    <span class="token comment"># label的形状：(batch_size,num_steps)</span>    <span class="token comment"># valid_len的形状：(batch_size,)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pred<span class="token punctuation">,</span> label<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span><span class="token punctuation">:</span>        weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>label<span class="token punctuation">)</span>        weights <span class="token operator">=</span> sequence_mask<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>reduction <span class="token operator">=</span> <span class="token string">'none'</span>        unweighted_loss <span class="token operator">=</span> <span class="token builtin">super</span><span class="token punctuation">(</span>MaskedSoftmaxCELoss<span class="token punctuation">,</span>                                self<span class="token punctuation">)</span><span class="token punctuation">.</span>forward<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token punctuation">)</span>        weighted_loss <span class="token operator">=</span> <span class="token punctuation">(</span>unweighted_loss <span class="token operator">*</span> weights<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> weighted_loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>反向传播的过程中,编码器和解码器就是通过那个context进行传递梯度.</p>]]></content>
    
    
    
    <tags>
      
      <tag>seq2seq</tag>
      
      <tag>rnn</tag>
      
      <tag>encoder-decoder</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2023/08/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF/"/>
    <url>/2023/08/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/mL_learning/2023-08-24-22-29-24.png"></p><p><img src="/img/mL_learning/2023-08-24-22-38-32.png"></p><p><img src="/img/mL_learning/2023-08-24-22-43-15.png"></p><p>残差网络</p><p><img src="/img/mL_learning/2023-08-24-22-58-02.png"><br><img src="/img/mL_learning/2023-08-24-23-15-12.png"></p><p>$\sigma(F(x)+x)$ 在这其中当$F(x) &#x3D; 0$时不会让训练效果变差,但是如果x中隐含信息,则会在残差块中学到一些有用的信息</p><p>目标检测</p><ul><li>确定输出格式</li><li>确定特征点</li><li>用一个小盒子在图像上滑动,每滑一次,将框住的内容拿去做分类,辨别是否有目标(效率很低)</li></ul><p><img src="/img/mL_learning/2023-08-26-20-30-45.png"></p><p><img src="/img/mL_learning/2023-08-26-20-43-49.png"></p><p><img src="/img/mL_learning/2023-08-26-20-53-30.png"></p><p><img src="/img/mL_learning/2023-08-26-20-57-17.png"></p><p><img src="/img/mL_learning/2023-08-26-23-24-45.png"><br><img src="/img/mL_learning/2023-08-26-23-25-07.png"><br><img src="/img/mL_learning/2023-08-26-23-25-43.png"></p><p><img src="/img/mL_learning/2023-08-26-23-33-20.png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>吴恩达机器学习week4--强化学习</title>
    <link href="/2023/07/29/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <url>/2023/07/29/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>定义: 强化学习不是给定指定的y来告诉程序怎么执行,而是设置奖励函数,当程序执行方向正确时给予奖励,错误时给予惩罚来进行修正程序的执行<br>过程</p><ul><li>状态</li><li>动作</li><li>奖励</li><li>下一个状态</li></ul><p>回报:<br><img src="/img/mL_learning/2023-07-29-15-44-31.png"><br>设置不同的gammaγ值<br><img src="/img/mL_learning/2023-07-29-15-53-13.png"></p><p>几个(MDP markov Decision Process 马可夫决策)的例子<br><img src="/img/mL_learning/2023-07-29-16-19-26.png"><br>MDP指的是未来的状态只与当前的状态有关,无关其他时候的任何状态</p><p>Q-function(Q函数)<br><img src="/img/mL_learning/2023-07-29-16-37-58.png"></p><p>贝尔曼方程<br><img src="/img/mL_learning/2023-07-29-17-17-05.png"></p><p>在强化学习过程中,状态的奖励都是随机的,因此找一个具体的最大值是没有意义的,因此改进一下贝尔曼方程<br>改成找后续状态的最大平均值(也就是期望E,在概率中用期望表示平均值)<br>为啥是期望呢,是因为强化学习过程中,智能体有概率走错方向<br><img src="/img/mL_learning/2023-07-29-17-55-09.png"></p><p>深度强化<br>输入状态输出$Q(s,a)$<br><img src="/img/mL_learning/2023-07-29-18-11-27.png"></p><p>DQN(Deep-Q-network)<br><img src="/img/mL_learning/2023-07-29-18-28-28.png"></p><p>优化方法</p><ul><li>修改神经网络的输出层,将一个神经元改成所有action的$Q(s,a)$</li><li>修改计算方法,引入$\epsilon$ (因为神经网络中的参数都是随机的,因此很可能出现某个行为的期望非常低,但是这实际上可能是个好的action,但是因为期望很低,智能体是不会选择它的,所以引入随机选择action的可能)(还可以在开始的时候将$\epsilon$设置比较高的值,随着算法运行,逐步减少$&#x2F;epsilon$)<br><img src="/img/mL_learning/2023-07-29-18-40-45.png"></li><li>Mini-batch</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>理解前向传播与后向传播的关系，串联神经网络流程</title>
    <link href="/2023/07/29/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <url>/2023/07/29/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/</url>
    
    <content type="html"><![CDATA[<p>在神经网络算法中，我们的训练集会有input和output(实际上就是y)<br>因此首先用前向传播获得${\hat y}$ 这个是预估的输出值,此时可以使用损失函数平方损失函数$J &#x3D; \frac{1}{2}{\sum_{i &#x3D; 1}^{m}(y-\hat y)^2}$来计算误差 或者其他损失函数(比如sigmoid函数的对数损失函数)<br>当误差比较大时就说明此时的参数$w$和$b$不合适,需要重新调整.<br>(这个后续让我想到了梯度下降)<br>以下面的神经网络为例</p><p><img src="/img/mL_learning/%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="神经网络图片"></p><p>计算$w_5$对整体结果的影响,使用$L$对$w_5$求偏导(为啥是求导嘞:因为输入是固定的,这里的$h_1$ 和 $h_2$都是上一层的输出层,结果已经由上一层确定了,所以$w, b$是变量)<br>在这里我们先求$L$对$w$和$b$的导数用于梯度下降更新$w和b$<br>令$ a &#x3D; \hat y$<br>$$<br>    z^{[l]} &#x3D; w^{[l]} \cdot a^{[l-1]} + b^{[l]}<br>$$<br>$$<br>    a^{[l]} &#x3D; \hat y &#x3D; \frac{1}{1+e^{-z^{[l]}}}<br>$$<br>$$<br>    L &#x3D; \frac{1}{2}(y_i - \hat {y_i})^2 &#x3D; \frac{1}{2}(y_i - a_i)^2<br>$$<br>$$<br>    \frac{\partial L}{\partial a_i} &#x3D; y_i - a_i<br>$$<br>$$<br>    \frac{\partial a_i}{\partial z_i} &#x3D; a_i \cdot (1-a_i)<br>$$</p><p>$$<br>    \frac{\partial z_i}{\partial w_i} &#x3D; x<br>$$<br>$$<br>    \frac{\partial J}{\partial w} &#x3D; \frac{\partial \frac{1}{2}{\sum_{i &#x3D; 1}^{m}(y-{\frac{1}{1+e^{-(x \cdot w+b)}}})}}{\partial w}&#x3D; \frac{\partial L}{\partial a}\cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w}<br>$$<br>$$<br>    \frac{\partial L}{\partial w} &#x3D; (y-a) \cdot a \cdot (1-a) \cdot x<br>$$<br>在上面的公式中带下标$i$的表示单个变量,不带下标的表示向量</p><p>接着使用梯度下降方程来更新w<br>$$<br>    w &#x3D; w-\alpha \cdot \frac{\partial J}{\partial w}<br>$$</p><p>$b$更新的式子同理,甚至比$w$简单,就不写了</p><p>这是倒数第一个隐藏层中参数$w$的更新公式,在倒数第二个隐藏层中的$w$ 则需要考虑到$L_1 和 L_2$对它都有影响这也只是多了几层链式求导罢了</p><p>在这里我给自己梳理了神经网络中前向传播和后向传播的关系,之前我一直割裂得看二者的关系,导致昨天听网课的时候完全听不懂这些关系式子是如何来的.</p><p>参考:<br>[1] 吴恩达深度学习网课<br>[2] <a href="https://www.cnblogs.com/charlotte77/p/5629865.html">https://www.cnblogs.com/charlotte77/p/5629865.html</a></p><p>[2]参考的是这篇文章:<br><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>前向传播</tag>
      
      <tag>后向传播</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>吴恩达深度学习NLP入门</title>
    <link href="/2023/07/23/NLP%E5%85%A5%E9%97%A8/"/>
    <url>/2023/07/23/NLP%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<p>RNN的出现<br>  传统的神经网络都是一个X对应一个Y,输入和输出一一对应,不同的输入之间没有关系.但是对于序列来说(文字序列,音频序列)一个个输入之间不是孤立的,而是有相对应关系的,因此引入RNN</p><p>RNN的公式统一定义为$h_t &#x3D; f(x_t, h_{t-1}; \theta)$<br>其中$h_t$的每一步计算都是由其当前的输入$x_t$和上一步的输入$h_{t-1}$决定的,$\theta$则是训练时使用的参数.<br>因此$\theta$是参数,因此在传播过程中就需要训练它使其成为合适的参数(前向传播+反向传播)<br>$$<br>\frac{\partial h_t}{\partial \theta} &#x3D; \frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial h_{t-1}}{\partial \theta} + \frac{\partial h_t}{\partial \theta}<br>$$<br>由上式中我们可以看到$\frac{\partial h_t}{\partial \theta}$是前一个状态$\frac{\partial h_{t-1}}{\partial \theta}$对$\theta$和当前状态对$\theta$的函数.在上面的式子中$|\frac{\partial h_t}{\partial h_{t-1}}|&gt;1$ 则表示增强前面的梯度, $\frac{\partial h_t}{\partial h_{t-1}}|&lt;1$则表示减弱前面的梯度.模型很难做到在1附近摆动,从而保持梯度信息,因此梯度消失或者梯度下降在所难免.</p><p><strong>对于梯度消失&#x2F;爆炸的理解</strong><br>在RNN中谈论梯度消失不是指总梯度&#x3D;0,而是对于前面的总项$\frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial h_{t-1}}{\partial \theta} \rarr o$ 这表示前面的梯度信息已经对当前梯度更新不起作用了, 在序列中表示前面的输入对后面输入的依赖关系已经消失了.</p><p>辅助公式<br>$$<br>tanh(x) &#x3D; 2\sigma(2x)-1<br>$$<br>$$<br>\sigma(x) &#x3D; \frac{1}{2}(tanh(\frac{x}{2})+1)<br>$$<br>$$<br>(tanhx)’ &#x3D; 1-{tanh}^2x<br>$$<br>$$<br>\sigma ‘ &#x3D; \sigma(x)(1-\sigma(x))<br>$$</p><p>原始RNN<br>$$<br>  h_t &#x3D; tanh(W_{x_t} + Uh_{t-1}+b)<br>$$<br>下面可以计算一下$ \frac{\partial h_t}{\partial h_{t-1}}$<br>$$<br> \frac{\partial h_t}{\partial h_{t-1}} &#x3D; (tanh(W_{x_t} + Uh_{t-1}+b))’\cdot U &#x3D; (1-{tanh}^2(W_{x_t} + Uh_{t-1}+b))*U &#x3D; (1-{h_t}^2)*U<br>$$<br>U的值是不确定的,因此$ \frac{\partial h_t}{\partial h_{t-1}}$的值可能大于1也可能小于1,因此是存在梯度消失&#x2F;爆炸的风险.因为激活函数使用tanh,因此当|U|非常大时,会将$h_t$限制在[-1,1]之间,使得$(1-{h_t}^2)*U$有界.<br>这也是使用tanh作为激活函数的原因,它能缓解梯度爆炸。如果我们使用relu激活函数时，当|U|非常大时，$h_t$也会非常大,$\frac{\partial h_t}{\partial h_{t-1}}$也很容易非常大,因此梯度爆炸风险非常高.而tanh激活函数可以将$h_t$限制在[-1,1]之间,大大降低了梯度爆炸的风险.</p><p>对于LSTM:<br>$$<br>f_t &#x3D; \sigma(W_{fx_t} + U_{f}h_{t-1}+b_f)<br>$$<br>$$<br>  i_t &#x3D; \sigma(W_{ix_t}+U_ih_{i-1}+b_i)<br>$$<br>$$<br>  o_t &#x3D; \sigma(W_{ox_t}+U_ch_{t-1}+b_c)<br>$$<br>$$<br>  \hat{c}<em>t &#x3D; tanh(W</em>{ox_t} + U_oh_{t-1}+b_c)<br>$$</p><p>$$<br>  c_t &#x3D; f_t \cdot c_{t-1} + i_t \cdot \hat{c}_t<br>$$<br>$$<br>  h_t &#x3D; o_t \cdot tanh(c_t)<br>$$</p><p>GRU:<br>$$<br>  z_t&#x3D;\sigma(W_{zx_t}+U_zh_{t-1}+b_z)<br>$$<br>$$<br>  r_t &#x3D; \sigma(W_{rx_t}+U_rh_{t-1}+b_r)<br>$$<br>$$<br>  \hat{h}<em>t &#x3D; tanh(W</em>{hx_t}+U_h(r_t\cdot h_{t-1})+b_c)<br>$$<br>$$<br>  h_t &#x3D; (1-z_t)\cdot h_{t-1}+z_t \cdot \hat h_t<br>$$</p><p><img src="/img/mL_learning/2023-08-29-21-34-52.png"></p><hr><p><img src="/img/mL_learning/2023-08-29-22-01-17.png"><br>W参数一样，激活函数中的参数不一样<br><img src="/img/mL_learning/2023-08-29-21-57-41.png"></p><p><img src="/img/mL_learning/2023-08-29-22-08-13.png"></p><p><img src="/img/mL_learning/2023-08-29-22-16-35.png"><br>a按时间顺序（倒序）来更新</p><p>梯度爆炸使用梯度修建的方式来缓解</p><p><img src="/img/mL_learning/2023-08-29-22-26-56.png"></p><p><img src="/img/mL_learning/2023-08-30-09-13-13.png"><br>由上图可以看出来, 后面得到的概率全是在前面的影响下得出来的.</p><p><img src="/img/mL_learning/2023-08-30-09-52-31.png"></p><p><img src="/img/mL_learning/2023-08-31-16-22-23.png"></p><p><img src="/img/mL_learning/2023-08-31-16-57-41.png"></p><p><img src="/img/mL_learning/2023-08-31-17-41-18.png"><br>还是没懂为什么使用余弦函数可以很好的预测词汇之间的关系</p><p><a href="https://www.zhihu.com/tardis/zm/art/43396514?source_id=1003">https://www.zhihu.com/tardis/zm/art/43396514?source_id=1003</a></p><p>对于ship-gram算法的实现过程不是很理解<br><a href="https://blog.csdn.net/weixin_41843918/article/details/90312339">https://blog.csdn.net/weixin_41843918/article/details/90312339</a></p><p>glove算法</p><p><img src="/img/mL_learning/2023-09-01-12-28-37.png"></p><p><img src="/img/mL_learning/2023-09-01-15-45-14.png"></p><p><img src="/img/mL_learning/2023-09-01-16-24-09.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>RNN</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/07/19/hello-world/"/>
    <url>/2023/07/19/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
