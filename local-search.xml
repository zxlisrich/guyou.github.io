<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>seq2seq模型的个人理解</title>
    <link href="/2023/11/26/seq2seq/"/>
    <url>/2023/11/26/seq2seq/</url>
    
    <content type="html"><![CDATA[<p><strong>这篇文章仅供个人学习理解，可能有错误</strong></p><h4 id="encoder-decoder架构"><a href="#encoder-decoder架构" class="headerlink" title="encoder-decoder架构"></a>encoder-decoder架构</h4><p>Encoder-Decoder架构是一种常见的深度学习模型架构，特别是在序列到序列的任务中。</p><ul><li><strong>Encoder</strong>：编码器的主要任务是理解和编码输入数据。在处理文本时，它通常会接收一些列单词，并将这些单词转换为一个连续的向量表示（中间形式），这个向量也被称为 <strong>context</strong>。这个过程可以看作是对输入信息进行压缩，并尽可能保留其核心含义。</li><li><strong>Decoder</strong>：解码器则负责从上述编码生成输出。具体来说，它会接收编码器产生的context，并逐步生成输出序列。每一步都会生成一个新元素，并将其作为下一步的输入之一。</li></ul><p>一个Encoder-Decoder结构为：</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""编码器-解码器架构的基类    Defined in :numref:`sec_encoder-decoder`"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_X<span class="token punctuation">,</span> dec_X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>        enc_outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span>        dec_state <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>init_state<span class="token punctuation">(</span>enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_X<span class="token punctuation">,</span> dec_state<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>在机器翻译任务中，enc_X是源语言, dec_X是对应的目标语言.<br>dec_X直接使用目标语言的原因:为了实现Teacher Forcing 策略.即在训练阶段,使用真实的数据+隐藏状态去预测输出而不是使用预测的输出去预测输出.(防止错上加错)</p><p>在seq2seq模型中,我们也是由编码器-解码器架构实现</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Seq2SeqEncoder</span><span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>Encoder<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""用于序列到序列学习的循环神经网络编码器"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 vocab_size<span class="token punctuation">,</span>                 embed_size<span class="token punctuation">,</span>                 num_hiddens<span class="token punctuation">,</span>                 num_layers<span class="token punctuation">,</span>                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                 <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Seq2SeqEncoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        <span class="token comment"># 嵌入层</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span>                         num_layers<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 输出'X'的形状：(batch_size,num_steps,embed_size)</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token comment"># 在循环神经网络模型中，第一个轴对应于时间步</span>        X <span class="token operator">=</span> X<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># 如果未提及状态，则默认为0</span>        output<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token comment"># output的形状:(num_steps,batch_size,num_hiddens)</span>        <span class="token comment"># state的形状:(num_layers,batch_size,num_hiddens)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>编码器中就正常使用RNN将输入的X变成中间形态.不过seq2seq模型中,解码器使用到的并不是output,而是编码器生成的state的最后一个时间步的隐藏状态,这个隐藏状态中浓缩了前面输入和隐藏状态的所有信息</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Seq2SeqDecoder</span><span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>Decoder<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""用于序列到序列学习的循环神经网络解码器"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                 vocab_size<span class="token punctuation">,</span>                 embed_size<span class="token punctuation">,</span>                 num_hiddens<span class="token punctuation">,</span>                 num_layers<span class="token punctuation">,</span>                 dropout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                 <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Seq2SeqDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embed_size <span class="token operator">+</span> num_hiddens<span class="token punctuation">,</span>                          num_hiddens<span class="token punctuation">,</span>                          num_layers<span class="token punctuation">,</span>                          dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">init_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> enc_outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 输出'X'的形状：(batch_size,num_steps,embed_size)</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># 广播context，使其具有与X相同的num_steps</span>        context <span class="token operator">=</span> state<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        X_and_context <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        output<span class="token punctuation">,</span> state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>X_and_context<span class="token punctuation">,</span> state<span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># output的形状:(batch_size,num_steps,vocab_size)</span>        <span class="token comment"># state的形状:(num_layers,batch_size,num_hiddens)</span>        <span class="token keyword">return</span> output<span class="token punctuation">,</span> state<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>在这个过程中损失函数通过继承CrossEntropyLoss重新定义一个损失函数类.因为在一个序列中,它有可能由实际文本+padding组成.因此我们需要去掉这个padding之后再去计算损失函数.</p><figure><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#@save</span><span class="token keyword">class</span> <span class="token class-name">MaskedSoftmaxCELoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""带遮蔽的softmax交叉熵损失函数"""</span>    <span class="token comment"># pred的形状：(batch_size,num_steps,vocab_size)</span>    <span class="token comment"># label的形状：(batch_size,num_steps)</span>    <span class="token comment"># valid_len的形状：(batch_size,)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pred<span class="token punctuation">,</span> label<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span><span class="token punctuation">:</span>        weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>label<span class="token punctuation">)</span>        weights <span class="token operator">=</span> sequence_mask<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> valid_len<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>reduction <span class="token operator">=</span> <span class="token string">'none'</span>        unweighted_loss <span class="token operator">=</span> <span class="token builtin">super</span><span class="token punctuation">(</span>MaskedSoftmaxCELoss<span class="token punctuation">,</span>                                self<span class="token punctuation">)</span><span class="token punctuation">.</span>forward<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token punctuation">)</span>        weighted_loss <span class="token operator">=</span> <span class="token punctuation">(</span>unweighted_loss <span class="token operator">*</span> weights<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> weighted_loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><p>反向传播的过程中,编码器和解码器就是通过那个context进行传递梯度.</p>]]></content>
    
    
    
    <tags>
      
      <tag>seq2seq</tag>
      
      <tag>rnn</tag>
      
      <tag>encoder-decoder</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>矩阵求导运算</title>
    <link href="/2023/09/07/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"/>
    <url>/2023/09/07/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/</url>
    
    <content type="html"><![CDATA[<p>###矩阵运算<br>遇见两次矩阵运算了，第一次在吴恩达那里就云里雾里的，现在决定好好搞懂它。</p><blockquote><h4 id="标量导数运算"><a href="#标量导数运算" class="headerlink" title="标量导数运算"></a>标量导数运算</h4><p>这个很简单,就是我们以前学过的导数计算,求的是斜率.</p><ul><li>$a’&#x3D;0$</li><li>$x’&#x3D;1$</li><li>$x^a&#x3D;ax^{a-1}$</li><li>…</li></ul></blockquote><blockquote><h4 id="向量导数运算"><a href="#向量导数运算" class="headerlink" title="向量导数运算"></a>向量导数运算</h4><p>当$x$为向量时,$f(\mathbf{x}) &#x3D; a_1x_1+a_2x_2+a_3x_3$<br>当$f$为向量时(此时x为标量),$ \mathbf{f}(x) &#x3D; [x+1, x^2, 2x+2]^T $,这里展示的是向量$\mathbf{f}$不同分量和$x$的关系<br>向量导数运算可以分为以下组合</p><table><thead><tr><th align="center">$x&#x2F;f(x)$</th><th align="center">标量</th><th align="left">向量</th></tr></thead><tbody><tr><td align="center">标量</td><td align="center">$f(x)$</td><td align="left">$\mathbf{f}(x)$</td></tr><tr><td align="center">向量</td><td align="center">$f(\mathbf{x})$</td><td align="left">$\mathbf{f}(\mathbf{x})$</td></tr><tr><td align="center">向量求导时就</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">$$</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">\mathbf{f}(\mathbf{x})&#x3D; \left[\begin{matrix} f_1(\mathbf{x})\f_2(\mathbf{x})\end{matrix}\right] &#x3D; \left[</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">\begin{matrix}</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">x_1+x_2\</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">x_1x_2^2</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">\end{matrix}</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">\right]</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">\tag{1}</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">$$</td><td align="center"></td><td align="left"></td></tr><tr><td align="center">$$\frac{\partial \mathbf{f}}{\partial \mathbf{x}} &#x3D; \left[\begin{matrix} \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2}\ \frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2}\end{matrix}\right] \tag{2}$$</td><td align="center"></td><td align="left"></td></tr></tbody></table><p>这里有两个问题没有需要解决</p><ul><li>导数的行列可以有不同的分法,这个的区别是什么</li><li>他们说向量求导就是找梯度,我现在暂时无法体会这是如何找到梯度的</li></ul></blockquote><blockquote><p>首先来解决第一个问题，导数的行列按不同的形式展开有什么区别.以(1)为例，它对$x$求导其实可以写成另一个形式<br>$$\frac{\partial \mathbf{f}}{\partial \mathbf{x}} &#x3D; \left[\begin{matrix} \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_1}\ \frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_1}{\partial x_2}\end{matrix}\right] \tag{3}$$<br>(2)为分子布局(分子为列向量形式,分母为行向量形式),又称为行向量偏导形式<br>而(3)则是分母布局(分子为行向量,分母为列向量),成为列向量偏导形式<br>刚体会了一下反向传播,在次过程中,如果使用分母布局,则会导致导数矩阵的维度和原矩阵的维度不一致,无法进行梯度下降计算.使用分子布局则会使导数矩阵和原矩阵维度一致.这应该就是深度学习中通常使用分子布局的原因.</p></blockquote><blockquote><p>第二个问题,其实这个也挺好理解的,导数矩阵到底还是由各个导数分量组成的,在进行梯度下降的过程中是分量-对应位置的分量,这不就是往下降方向最大变化吗</p></blockquote><blockquote><h4 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h4><p>我个人是感觉矩阵运算和向量运算非常类似,区别就是分量多了点…</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>矩阵运算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2023/08/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF/"/>
    <url>/2023/08/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%B7%E7%A7%AF/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/mL_learning/2023-08-24-22-29-24.png"></p><p><img src="/img/mL_learning/2023-08-24-22-38-32.png"></p><p><img src="/img/mL_learning/2023-08-24-22-43-15.png"></p><p>残差网络</p><p><img src="/img/mL_learning/2023-08-24-22-58-02.png"><br><img src="/img/mL_learning/2023-08-24-23-15-12.png"></p><p>$\sigma(F(x)+x)$ 在这其中当$F(x) &#x3D; 0$时不会让训练效果变差,但是如果x中隐含信息,则会在残差块中学到一些有用的信息</p><p>目标检测</p><ul><li>确定输出格式</li><li>确定特征点</li><li>用一个小盒子在图像上滑动,每滑一次,将框住的内容拿去做分类,辨别是否有目标(效率很低)</li></ul><p><img src="/img/mL_learning/2023-08-26-20-30-45.png"></p><p><img src="/img/mL_learning/2023-08-26-20-43-49.png"></p><p><img src="/img/mL_learning/2023-08-26-20-53-30.png"></p><p><img src="/img/mL_learning/2023-08-26-20-57-17.png"></p><p><img src="/img/mL_learning/2023-08-26-23-24-45.png"><br><img src="/img/mL_learning/2023-08-26-23-25-07.png"><br><img src="/img/mL_learning/2023-08-26-23-25-43.png"></p><p><img src="/img/mL_learning/2023-08-26-23-33-20.png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>吴恩达机器学习week4--强化学习</title>
    <link href="/2023/07/29/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <url>/2023/07/29/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>定义: 强化学习不是给定指定的y来告诉程序怎么执行,而是设置奖励函数,当程序执行方向正确时给予奖励,错误时给予惩罚来进行修正程序的执行<br>过程</p><ul><li>状态</li><li>动作</li><li>奖励</li><li>下一个状态</li></ul><p>回报:<br><img src="/img/mL_learning/2023-07-29-15-44-31.png"><br>设置不同的gammaγ值<br><img src="/img/mL_learning/2023-07-29-15-53-13.png"></p><p>几个(MDP markov Decision Process 马可夫决策)的例子<br><img src="/img/mL_learning/2023-07-29-16-19-26.png"><br>MDP指的是未来的状态只与当前的状态有关,无关其他时候的任何状态</p><p>Q-function(Q函数)<br><img src="/img/mL_learning/2023-07-29-16-37-58.png"></p><p>贝尔曼方程<br><img src="/img/mL_learning/2023-07-29-17-17-05.png"></p><p>在强化学习过程中,状态的奖励都是随机的,因此找一个具体的最大值是没有意义的,因此改进一下贝尔曼方程<br>改成找后续状态的最大平均值(也就是期望E,在概率中用期望表示平均值)<br>为啥是期望呢,是因为强化学习过程中,智能体有概率走错方向<br><img src="/img/mL_learning/2023-07-29-17-55-09.png"></p><p>深度强化<br>输入状态输出$Q(s,a)$<br><img src="/img/mL_learning/2023-07-29-18-11-27.png"></p><p>DQN(Deep-Q-network)<br><img src="/img/mL_learning/2023-07-29-18-28-28.png"></p><p>优化方法</p><ul><li>修改神经网络的输出层,将一个神经元改成所有action的$Q(s,a)$</li><li>修改计算方法,引入$\epsilon$ (因为神经网络中的参数都是随机的,因此很可能出现某个行为的期望非常低,但是这实际上可能是个好的action,但是因为期望很低,智能体是不会选择它的,所以引入随机选择action的可能)(还可以在开始的时候将$\epsilon$设置比较高的值,随着算法运行,逐步减少$&#x2F;epsilon$)<br><img src="/img/mL_learning/2023-07-29-18-40-45.png"></li><li>Mini-batch</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>理解前向传播与后向传播的关系，串联神经网络流程</title>
    <link href="/2023/07/29/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <url>/2023/07/29/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/</url>
    
    <content type="html"><![CDATA[<p>在神经网络算法中，我们的训练集会有input和output(实际上就是y)<br>因此首先用前向传播获得${\hat y}$ 这个是预估的输出值,此时可以使用损失函数平方损失函数$J &#x3D; \frac{1}{2}{\sum_{i &#x3D; 1}^{m}(y-\hat y)^2}$来计算误差 或者其他损失函数(比如sigmoid函数的对数损失函数)<br>当误差比较大时就说明此时的参数$w$和$b$不合适,需要重新调整.<br>(这个后续让我想到了梯度下降)<br>以下面的神经网络为例</p><p><img src="/img/mL_learning/%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="神经网络图片"></p><p>计算$w_5$对整体结果的影响,使用$L$对$w_5$求偏导(为啥是求导嘞:因为输入是固定的,这里的$h_1$ 和 $h_2$都是上一层的输出层,结果已经由上一层确定了,所以$w, b$是变量)<br>在这里我们先求$L$对$w$和$b$的导数用于梯度下降更新$w和b$<br>令$ a &#x3D; \hat y$<br>$$<br>    z^{[l]} &#x3D; w^{[l]} \cdot a^{[l-1]} + b^{[l]}<br>$$<br>$$<br>    a^{[l]} &#x3D; \hat y &#x3D; \frac{1}{1+e^{-z^{[l]}}}<br>$$<br>$$<br>    L &#x3D; \frac{1}{2}(y_i - \hat {y_i})^2 &#x3D; \frac{1}{2}(y_i - a_i)^2<br>$$<br>$$<br>    \frac{\partial L}{\partial a_i} &#x3D; y_i - a_i<br>$$<br>$$<br>    \frac{\partial a_i}{\partial z_i} &#x3D; a_i \cdot (1-a_i)<br>$$</p><p>$$<br>    \frac{\partial z_i}{\partial w_i} &#x3D; x<br>$$<br>$$<br>    \frac{\partial J}{\partial w} &#x3D; \frac{\partial \frac{1}{2}{\sum_{i &#x3D; 1}^{m}(y-{\frac{1}{1+e^{-(x \cdot w+b)}}})}}{\partial w}&#x3D; \frac{\partial L}{\partial a}\cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w}<br>$$<br>$$<br>    \frac{\partial L}{\partial w} &#x3D; (y-a) \cdot a \cdot (1-a) \cdot x<br>$$<br>在上面的公式中带下标$i$的表示单个变量,不带下标的表示向量</p><p>接着使用梯度下降方程来更新w<br>$$<br>    w &#x3D; w-\alpha \cdot \frac{\partial J}{\partial w}<br>$$</p><p>$b$更新的式子同理,甚至比$w$简单,就不写了</p><p>这是倒数第一个隐藏层中参数$w$的更新公式,在倒数第二个隐藏层中的$w$ 则需要考虑到$L_1 和 L_2$对它都有影响这也只是多了几层链式求导罢了</p><p>在这里我给自己梳理了神经网络中前向传播和后向传播的关系,之前我一直割裂得看二者的关系,导致昨天听网课的时候完全听不懂这些关系式子是如何来的.</p><p>参考:<br>[1] 吴恩达深度学习网课<br>[2] <a href="https://www.cnblogs.com/charlotte77/p/5629865.html">https://www.cnblogs.com/charlotte77/p/5629865.html</a></p><p>[2]参考的是这篇文章:<br><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>前向传播</tag>
      
      <tag>后向传播</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>月球绕地球旋转scss实现</title>
    <link href="/2023/07/23/moon-and-earth/"/>
    <url>/2023/07/23/moon-and-earth/</url>
    
    <content type="html"><![CDATA[<p>目前遇到的问题：</p><ul><li>文件嵌套不太合适，导致css代码很长</li><li>暂停运动部分有点问题，必须加上<code>!important</code> 地球才会停止自转</li><li>月球的样子有点问题，它不是个球体形状，是个扁的</li></ul><p>直接贴代码</p><figure><div class="code-wrapper"><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token doctype"><span class="token punctuation">&lt;!</span><span class="token doctype-tag">DOCTYPE</span> <span class="token name">html</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>html</span> <span class="token attr-name">lang</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>en<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>head</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>meta</span> <span class="token attr-name">charset</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>UTF-8<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>meta</span> <span class="token attr-name">http-equiv</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>X-UA-Compatible<span class="token punctuation">"</span></span> <span class="token attr-name">content</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>IE=edge<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>meta</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>viewport<span class="token punctuation">"</span></span> <span class="token attr-name">content</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>width=device-width, initial-scale=1.0<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>title</span><span class="token punctuation">></span></span>Document<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>title</span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>style</span><span class="token punctuation">></span></span><span class="token style"><span class="token language-css">        <span class="token atrule"><span class="token rule">@import</span> <span class="token url"><span class="token function">url</span><span class="token punctuation">(</span>box.css<span class="token punctuation">)</span></span><span class="token punctuation">;</span></span>   </span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>style</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>head</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>body</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>box<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>bigBox center<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>boximg center<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>toMove center<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>./img/img2.jpg<span class="token punctuation">"</span></span> <span class="token attr-name">alt</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>img1<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>span</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>./img/img2.jpg<span class="token punctuation">"</span></span> <span class="token attr-name">alt</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>img2<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>span</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>moonBox<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>line<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>moon<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>body</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>html</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure><figure><div class="code-wrapper"><pre class="line-numbers language-css" data-language="css"><code class="language-css"><span class="token selector">.box</span> <span class="token punctuation">&#123;</span>  <span class="token property">width</span><span class="token punctuation">:</span> 400px<span class="token punctuation">;</span>  <span class="token property">height</span><span class="token punctuation">:</span> 400px<span class="token punctuation">;</span>  <span class="token property">margin</span><span class="token punctuation">:</span> 100px auto<span class="token punctuation">;</span>  <span class="token property">position</span><span class="token punctuation">:</span> relative<span class="token punctuation">;</span>  <span class="token selector">.center</span> <span class="token punctuation">&#123;</span>    <span class="token property">display</span><span class="token punctuation">:</span> flex<span class="token punctuation">;</span>    <span class="token property">align-items</span><span class="token punctuation">:</span> center<span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span>  <span class="token selector">.bigBox</span> <span class="token punctuation">&#123;</span>    <span class="token property">height</span><span class="token punctuation">:</span> 100%<span class="token punctuation">;</span>    <span class="token property">background-color</span><span class="token punctuation">:</span> #000<span class="token punctuation">;</span>    <span class="token property">box-sizing</span><span class="token punctuation">:</span> border-box<span class="token punctuation">;</span>    <span class="token property">justify-content</span><span class="token punctuation">:</span> center<span class="token punctuation">;</span>    <span class="token selector">.boximg</span> <span class="token punctuation">&#123;</span>      <span class="token property">width</span><span class="token punctuation">:</span> 250px<span class="token punctuation">;</span>      <span class="token property">height</span><span class="token punctuation">:</span> 250px<span class="token punctuation">;</span>      <span class="token property">border-radius</span><span class="token punctuation">:</span> 50%<span class="token punctuation">;</span>      <span class="token property">background-color</span><span class="token punctuation">:</span> <span class="token function">rgb</span><span class="token punctuation">(</span>40<span class="token punctuation">,</span> 123<span class="token punctuation">,</span> 171<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token property">overflow</span><span class="token punctuation">:</span> hidden<span class="token punctuation">;</span>      <span class="token property">position</span><span class="token punctuation">:</span> relative<span class="token punctuation">;</span>      <span class="token selector">.toMove</span> <span class="token punctuation">&#123;</span>        <span class="token property">height</span><span class="token punctuation">:</span> 100%<span class="token punctuation">;</span>        <span class="token property">flex-direction</span><span class="token punctuation">:</span> row<span class="token punctuation">;</span>        <span class="token property">justify-content</span><span class="token punctuation">:</span> start<span class="token punctuation">;</span>        <span class="token property">position</span><span class="token punctuation">:</span> absolute<span class="token punctuation">;</span>        <span class="token property">animation</span><span class="token punctuation">:</span> move 5s linear 0s infinite<span class="token punctuation">;</span>        <span class="token selector">img</span> <span class="token punctuation">&#123;</span>          <span class="token property">width</span><span class="token punctuation">:</span> 500px<span class="token punctuation">;</span>          <span class="token property">height</span><span class="token punctuation">:</span> 150px<span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>      <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span>  <span class="token selector">.moonBox</span> <span class="token punctuation">&#123;</span>    <span class="token property">width</span><span class="token punctuation">:</span> 280px<span class="token punctuation">;</span>    <span class="token property">height</span><span class="token punctuation">:</span> 280px<span class="token punctuation">;</span>    <span class="token property">position</span><span class="token punctuation">:</span> absolute<span class="token punctuation">;</span>    <span class="token property">top</span><span class="token punctuation">:</span> 10%<span class="token punctuation">;</span>    <span class="token property">left</span><span class="token punctuation">:</span> 15%<span class="token punctuation">;</span>    <span class="token property">border-radius</span><span class="token punctuation">:</span> 50%<span class="token punctuation">;</span>    <span class="token property">background-color</span><span class="token punctuation">:</span> transparent<span class="token punctuation">;</span>    <span class="token property">border-bottom</span><span class="token punctuation">:</span> 2px solid #fff<span class="token punctuation">;</span>    <span class="token property">border-left</span><span class="token punctuation">:</span> 2px solid #fff<span class="token punctuation">;</span>    <span class="token property">transform</span><span class="token punctuation">:</span> <span class="token function">skew</span><span class="token punctuation">(</span>45deg<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token selector">.line</span> <span class="token punctuation">&#123;</span>      <span class="token property">width</span><span class="token punctuation">:</span> 100%<span class="token punctuation">;</span>      <span class="token property">height</span><span class="token punctuation">:</span> 100%<span class="token punctuation">;</span>      <span class="token property">animation</span><span class="token punctuation">:</span> rotate-moon 2s linear infinite<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token selector">.moon</span> <span class="token punctuation">&#123;</span>      <span class="token property">width</span><span class="token punctuation">:</span> 30px<span class="token punctuation">;</span>      <span class="token property">height</span><span class="token punctuation">:</span> 30px<span class="token punctuation">;</span>      <span class="token property">border</span><span class="token punctuation">:</span> 5px solid #c0c0c0<span class="token punctuation">;</span>      <span class="token property">border-radius</span><span class="token punctuation">:</span> 50%<span class="token punctuation">;</span>      <span class="token property">background-color</span><span class="token punctuation">:</span> #ffffff<span class="token punctuation">;</span>      <span class="token property">position</span><span class="token punctuation">:</span> absolute<span class="token punctuation">;</span>      <span class="token property">top</span><span class="token punctuation">:</span> 92%<span class="token punctuation">;</span>      <span class="token property">left</span><span class="token punctuation">:</span> 45%<span class="token punctuation">;</span>      // <span class="token property">transform</span><span class="token punctuation">:</span> <span class="token function">skew</span><span class="token punctuation">(</span>-45deg<span class="token punctuation">)</span> <span class="token important">!important</span><span class="token punctuation">;</span>      <span class="token property">animation</span><span class="token punctuation">:</span> scale-moon 2s linear infinite<span class="token punctuation">;</span>      <span class="token property">box-shadow</span><span class="token punctuation">:</span> 0 0 30px 10px <span class="token function">rgba</span><span class="token punctuation">(</span>255<span class="token punctuation">,</span> 255<span class="token punctuation">,</span> 255<span class="token punctuation">,</span> 0.7<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span>  <span class="token selector">&amp;:hover</span> <span class="token punctuation">&#123;</span>    <span class="token selector">.toMove, .line, .moon</span> <span class="token punctuation">&#123;</span>      <span class="token property">animation-play-state</span><span class="token punctuation">:</span> paused <span class="token important">!important</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token atrule"><span class="token rule">@keyframes</span> rotate-moon</span> <span class="token punctuation">&#123;</span>  <span class="token selector">from</span> <span class="token punctuation">&#123;</span>    <span class="token property">transform</span><span class="token punctuation">:</span>  <span class="token function">rotate</span><span class="token punctuation">(</span>0deg<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span>  <span class="token selector">to</span> <span class="token punctuation">&#123;</span>    <span class="token property">transform</span><span class="token punctuation">:</span> <span class="token function">rotate</span><span class="token punctuation">(</span>360deg<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token atrule"><span class="token rule">@keyframes</span> move</span> <span class="token punctuation">&#123;</span>  <span class="token selector">0%</span> <span class="token punctuation">&#123;</span>    <span class="token property">left</span><span class="token punctuation">:</span> -50px<span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span>  <span class="token selector">100%</span> <span class="token punctuation">&#123;</span>    <span class="token property">left</span><span class="token punctuation">:</span> -450px<span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token atrule"><span class="token rule">@keyframes</span> scale-moon</span> <span class="token punctuation">&#123;</span>  <span class="token selector">from</span> <span class="token punctuation">&#123;</span>    <span class="token property">transform</span><span class="token punctuation">:</span> <span class="token function">scale</span><span class="token punctuation">(</span>1<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span>  <span class="token selector">70%</span> <span class="token punctuation">&#123;</span>    <span class="token property">transform</span><span class="token punctuation">:</span> <span class="token function">scale</span><span class="token punctuation">(</span>0.3<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span>  <span class="token selector">to</span> <span class="token punctuation">&#123;</span>    <span class="token property">transform</span><span class="token punctuation">:</span> <span class="token function">scale</span><span class="token punctuation">(</span>1<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div></figure>]]></content>
    
    
    
    <tags>
      
      <tag>前端</tag>
      
      <tag>HTML</tag>
      
      <tag>SASS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>吴恩达机器学习week4</title>
    <link href="/2023/07/23/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0week/"/>
    <url>/2023/07/23/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0week/</url>
    
    <content type="html"><![CDATA[<h4 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h4><blockquote><p>根节点和决策节点都是代表特征，<br>决策 </p></blockquote><h4 id="entropy-熵"><a href="#entropy-熵" class="headerlink" title="entropy (熵)"></a>entropy (熵)</h4><blockquote><p>如何选择特征会使熵最小</p><ul><li>使用加权平均熵 选择平均熵最小的分配方式<br><img src="/img/mL_learning/2023-07-25-22-14-15.png"></li><li>使用划分之前的熵减去划分之后的熵 得到结果与上述一致,但是可以看出谁纯度提升大 这个差值成为信息增益<br><img src="/img/mL_learning/2023-07-25-22-24-42.png" alt="信息增益的计算"></li></ul></blockquote><p>为啥要使用加权平均熵计算来着??? 应该只是为了综合两边的熵一起对比吧</p><blockquote><p>选择信息增益最大的特征进行划分,划分结束依据</p><ul><li>当一个节点占比100%</li><li>划分的深度达到最大(深度如何确定)</li><li>信息增益的增加幅度达到了阈值(再划分容易过拟合) (阈值如何确定)</li></ul></blockquote><blockquote><p>当特征有两个以上的离散值(比如毛色有白色,褐色,三花)<br>one-hot ???<br><img src="/img/mL_learning/2023-07-25-22-54-35.png"></p></blockquote><p>可以理解成基于范围去做二分类问题（sigmoid），计算的损失函数就是上下节点的 熵差，求maximize（熵差(x)），然后得到0和1，用于one-hot</p><p>回归树???? </p><p>通过建立多个决策树来保证决策树算法的健壮性<br>然而如何建立略有不同的多棵决策树呢？<br>即有放回抽样<br>随机森林算法<br>XGBoost </p><p>决策树算法</p><ul><li>tabular（structured) data</li><li>不推荐在非结构化数据（图标，音频等）上面使用决策树算法<br>对比神经网络</li><li>适用于所有类型的数据</li><li>但是可能慢于决策树算法</li><li>可以使用迁移学习</li><li>更加容易构建多模型系统</li></ul><h4 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h4><ul><li>(clustering)聚类<ul><li>grouping similar news </li><li>DNA analysis</li><li>Astronomical data analysis</li></ul></li><li>K-means算法<ul><li>随机初始化K个聚类中心</li><li>loop <ul><li>计算每个点到不同聚类中心的距离,将其分配到距离最短的聚类中心</li><li>全部分配完成后根据各个聚类计算平均聚类中心</li></ul></li></ul></li><li>优化算法(cost function &#x3D;&#x3D; distortion function)<br><img src="/img/mL_learning/2023-07-26-23-35-39.png"></li><li>如何决定找几个集合</li></ul><p><img src="/img/mL_learning/2023-07-27-22-51-29.png"><br>相当于用无标注的训练集训练出一个特定均值和方差的正态分布，并默认两端的极值是不正常的。再通过测试集来调整阈值，使得阈值之上的都是正常的，阈值之外的都是不正常的。</p><p>在异常检测算法中选择合适的特征<br>在一些特征的概率分布不是高斯分布时 可以尝试使用变换手段使其变换为高斯分布<br><img src="/img/mL_learning/2023-07-27-23-09-08.png"></p><p>推荐系统<br><img src="/img/mL_learning/2023-07-27-23-27-00.png"></p><p>预测Alice对其他电影的评分的方式（类似线性回归）<br><img src="/img/mL_learning/2023-07-27-23-33-09.png"><br>cost function 也一样（但是只算上实际评分的个数）<br><img src="/img/mL_learning/2023-07-27-23-37-32.png"></p><p>如何得出X1 和 X2 这类特征</p><p>太怪了，这里假设参数 w和b 求特征X，但是这里又把w, b, X整合到一个公式里面<br><img src="/img/mL_learning/2023-07-29-10-16-16.png"></p><p>意思是使用三个变量同时做梯度下降,然后使上面第三个式子最小<br><img src="/img/mL_learning/2023-07-29-10-36-57.png"></p><p>将标签简化(用户并不都是给予五星好评的,他们通常只是通过某种途径让你感觉她喜欢或者不喜欢这个电影)</p><ul><li>用户看过电影并评价了标为1 </li><li>知道,可能看过,可能不喜欢没有给予评分标为0</li><li>没看过标为?<br>将上面的线性回归转为分类的逻辑回归流程如下<br><img src="/img/mL_learning/2023-07-29-10-54-52.png"></li></ul><p>(已经回到线性回归了)<br>当存在没怎么看过电影的用户,使用均值归一化可以达到加快协同过滤算法运算的效果<br><img src="/img/mL_learning/2023-07-29-11-31-39.png"></p><p>相关特征<br>(解释的例子, 当浏览商品时,淘宝根据你浏览的商品的特征去找到欧式距离小的特征,然后根据这些特征给你推荐其他商品)<br><img src="/img/mL_learning/2023-07-29-11-52-26.png"></p><p>协同过滤算法的局限<br>冷启动问题:初始时,用户数据非常少,不能很好的给用户推荐内容<br>需要关于用户和项目的辅助信息<br><img src="/img/mL_learning/2023-07-29-11-54-33.png"></p><p>协同过滤算法和基于内容的推荐算法对比</p><ul><li>协同过滤算法是根据与你相似的用户的信息给你进行推荐</li><li>基于内容的过滤算法是将项目特征与用户特征进行匹配,寻找关系来进行推荐内容<br><img src="/img/mL_learning/2023-07-29-12-01-46.png"></li></ul><p>基于内容的推荐算法</p><ul><li>根据用户特征$X_u$ 计算出向量$V_u$ 根据项目特征$X_m$ 计算出向量$V_m$<ul><li>计算方式是神经网络<br><img src="/img/mL_learning/2023-07-29-12-20-45.png"></li><li>两神经网络整合在一起(使用平方损失函数来找到合适的输出层)<br><img src="/img/mL_learning/2023-07-29-12-24-26.png"></li></ul></li></ul><p>基于内容的推荐算法来推荐类似的电影使用的方法类似协同过滤算法<br><img src="/img/mL_learning/2023-07-29-12-30-20.png"></p><p>特征降维</p><ul><li>PCA</li></ul><p>$J &#x3D; -\frac{1}{m}\sum_{i&#x3D;1}^{m}(y*loga +(1-y)*log(1-a) )$</p>]]></content>
    
    
    
    <tags>
      
      <tag>决策树</tag>
      
      <tag>信息熵</tag>
      
      <tag>信息增益</tag>
      
      <tag>推荐系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>吴恩达深度学习NLP入门</title>
    <link href="/2023/07/23/NLP%E5%85%A5%E9%97%A8/"/>
    <url>/2023/07/23/NLP%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<p>RNN的出现<br>  传统的神经网络都是一个X对应一个Y,输入和输出一一对应,不同的输入之间没有关系.但是对于序列来说(文字序列,音频序列)一个个输入之间不是孤立的,而是有相对应关系的,因此引入RNN</p><p>RNN的公式统一定义为$h_t &#x3D; f(x_t, h_{t-1}; \theta)$<br>其中$h_t$的每一步计算都是由其当前的输入$x_t$和上一步的输入$h_{t-1}$决定的,$\theta$则是训练时使用的参数.<br>因此$\theta$是参数,因此在传播过程中就需要训练它使其成为合适的参数(前向传播+反向传播)<br>$$<br>\frac{\partial h_t}{\partial \theta} &#x3D; \frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial h_{t-1}}{\partial \theta} + \frac{\partial h_t}{\partial \theta}<br>$$<br>由上式中我们可以看到$\frac{\partial h_t}{\partial \theta}$是前一个状态$\frac{\partial h_{t-1}}{\partial \theta}$对$\theta$和当前状态对$\theta$的函数.在上面的式子中$|\frac{\partial h_t}{\partial h_{t-1}}|&gt;1$ 则表示增强前面的梯度, $\frac{\partial h_t}{\partial h_{t-1}}|&lt;1$则表示减弱前面的梯度.模型很难做到在1附近摆动,从而保持梯度信息,因此梯度消失或者梯度下降在所难免.</p><p><strong>对于梯度消失&#x2F;爆炸的理解</strong><br>在RNN中谈论梯度消失不是指总梯度&#x3D;0,而是对于前面的总项$\frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial h_{t-1}}{\partial \theta} \rarr o$ 这表示前面的梯度信息已经对当前梯度更新不起作用了, 在序列中表示前面的输入对后面输入的依赖关系已经消失了.</p><p>辅助公式<br>$$<br>tanh(x) &#x3D; 2\sigma(2x)-1<br>$$<br>$$<br>\sigma(x) &#x3D; \frac{1}{2}(tanh(\frac{x}{2})+1)<br>$$<br>$$<br>(tanhx)’ &#x3D; 1-{tanh}^2x<br>$$<br>$$<br>\sigma ‘ &#x3D; \sigma(x)(1-\sigma(x))<br>$$</p><p>原始RNN<br>$$<br>  h_t &#x3D; tanh(W_{x_t} + Uh_{t-1}+b)<br>$$<br>下面可以计算一下$ \frac{\partial h_t}{\partial h_{t-1}}$<br>$$<br> \frac{\partial h_t}{\partial h_{t-1}} &#x3D; (tanh(W_{x_t} + Uh_{t-1}+b))’\cdot U &#x3D; (1-{tanh}^2(W_{x_t} + Uh_{t-1}+b))*U &#x3D; (1-{h_t}^2)*U<br>$$<br>U的值是不确定的,因此$ \frac{\partial h_t}{\partial h_{t-1}}$的值可能大于1也可能小于1,因此是存在梯度消失&#x2F;爆炸的风险.因为激活函数使用tanh,因此当|U|非常大时,会将$h_t$限制在[-1,1]之间,使得$(1-{h_t}^2)*U$有界.<br>这也是使用tanh作为激活函数的原因,它能缓解梯度爆炸。如果我们使用relu激活函数时，当|U|非常大时，$h_t$也会非常大,$\frac{\partial h_t}{\partial h_{t-1}}$也很容易非常大,因此梯度爆炸风险非常高.而tanh激活函数可以将$h_t$限制在[-1,1]之间,大大降低了梯度爆炸的风险.</p><p>对于LSTM:<br>$$<br>f_t &#x3D; \sigma(W_{fx_t} + U_{f}h_{t-1}+b_f)<br>$$<br>$$<br>  i_t &#x3D; \sigma(W_{ix_t}+U_ih_{i-1}+b_i)<br>$$<br>$$<br>  o_t &#x3D; \sigma(W_{ox_t}+U_ch_{t-1}+b_c)<br>$$<br>$$<br>  \hat{c}<em>t &#x3D; tanh(W</em>{ox_t} + U_oh_{t-1}+b_c)<br>$$</p><p>$$<br>  c_t &#x3D; f_t \cdot c_{t-1} + i_t \cdot \hat{c}_t<br>$$<br>$$<br>  h_t &#x3D; o_t \cdot tanh(c_t)<br>$$</p><p>GRU:<br>$$<br>  z_t&#x3D;\sigma(W_{zx_t}+U_zh_{t-1}+b_z)<br>$$<br>$$<br>  r_t &#x3D; \sigma(W_{rx_t}+U_rh_{t-1}+b_r)<br>$$<br>$$<br>  \hat{h}<em>t &#x3D; tanh(W</em>{hx_t}+U_h(r_t\cdot h_{t-1})+b_c)<br>$$<br>$$<br>  h_t &#x3D; (1-z_t)\cdot h_{t-1}+z_t \cdot \hat h_t<br>$$</p><p><img src="/img/mL_learning/2023-08-29-21-34-52.png"></p><hr><p><img src="/img/mL_learning/2023-08-29-22-01-17.png"><br>W参数一样，激活函数中的参数不一样<br><img src="/img/mL_learning/2023-08-29-21-57-41.png"></p><p><img src="/img/mL_learning/2023-08-29-22-08-13.png"></p><p><img src="/img/mL_learning/2023-08-29-22-16-35.png"><br>a按时间顺序（倒序）来更新</p><p>梯度爆炸使用梯度修建的方式来缓解</p><p><img src="/img/mL_learning/2023-08-29-22-26-56.png"></p><p><img src="/img/mL_learning/2023-08-30-09-13-13.png"><br>由上图可以看出来, 后面得到的概率全是在前面的影响下得出来的.</p><p><img src="/img/mL_learning/2023-08-30-09-52-31.png"></p><p><img src="/img/mL_learning/2023-08-31-16-22-23.png"></p><p><img src="/img/mL_learning/2023-08-31-16-57-41.png"></p><p><img src="/img/mL_learning/2023-08-31-17-41-18.png"><br>还是没懂为什么使用余弦函数可以很好的预测词汇之间的关系</p><p><a href="https://www.zhihu.com/tardis/zm/art/43396514?source_id=1003">https://www.zhihu.com/tardis/zm/art/43396514?source_id=1003</a></p><p>对于ship-gram算法的实现过程不是很理解<br><a href="https://blog.csdn.net/weixin_41843918/article/details/90312339">https://blog.csdn.net/weixin_41843918/article/details/90312339</a></p><p>glove算法</p><p><img src="/img/mL_learning/2023-09-01-12-28-37.png"></p><p><img src="/img/mL_learning/2023-09-01-15-45-14.png"></p><p><img src="/img/mL_learning/2023-09-01-16-24-09.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>RNN</tag>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2023/07/19/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2023/07/19/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/07/19/hello-world/"/>
    <url>/2023/07/19/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure><div class="code-wrapper"><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
